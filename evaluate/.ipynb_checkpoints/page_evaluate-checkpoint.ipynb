{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee74834-cd91-46f4-b630-238ac54b272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import load_simple_json,load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9728443-2b4f-4fe1-b80c-8d86006d02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction(pred,thresh=0.5,topk=None):\n",
    "    '''\n",
    "    input: raw prediction\n",
    "    return: dictionary with id and page value\n",
    "    '''\n",
    "    re_dic={}\n",
    "    for qid in pred:\n",
    "        buf_lst=[]\n",
    "        scores = pred[qid][\"score\"]\n",
    "        if topk:\n",
    "            sorted_lst = sorted(scores,reverse=True)[:topk]\n",
    "            [buf_lst.append(pred[qid][\"page_ids\"][scores.index(i)]) for i in sorted_lst]\n",
    "        else:\n",
    "            [buf_lst.append(pred[qid][\"page_ids\"][idx]) for idx,value in \n",
    "                    enumerate(scores) if value >= thresh]\n",
    "        re_dic[qid]=buf_lst\n",
    "    return re_dic\n",
    "def join_data_with_preds(ori_data,preds):\n",
    "    '''\n",
    "    input: original data and processed predcition\n",
    "    return: joint data with prediciton\n",
    "    '''\n",
    "    re_lst=[]\n",
    "    for i in ori_data:\n",
    "        id_buf = str(i[\"id\"])\n",
    "        if id_buf in preds.keys():\n",
    "            i[\"predicted_pages_1\"]=preds[id_buf]\n",
    "            re_lst.append(i)\n",
    "    return re_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20dd55e6-b24b-4a59-bc83-65fac38f377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(data,predictions: pd.Series) -> None:\n",
    "    precision = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        # Extract all ground truth of titles of the wikipedia pages\n",
    "        # evidence[2] refers to the title of the wikipedia page\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        if len(predicted_pages) != 0:\n",
    "            precision += len(hits) / len(predicted_pages)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Macro precision\n",
    "    print(f\"Precision: {precision / count}\")\n",
    "def at_least_get_one(data,predictions: pd.Series) -> None:\n",
    "    precision = 0\n",
    "    count = 0\n",
    "    hit_counts=0\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        # Extract all ground truth of titles of the wikipedia pages\n",
    "        # evidence[2] refers to the title of the wikipedia page\n",
    "        at_least_get_one = False\n",
    "        for evidence_set in d[\"evidence\"]:\n",
    "            evid_buf=[]\n",
    "            for evidence in evidence_set:\n",
    "                evid_buf.append(evidence[2])\n",
    "            hit_count = 0\n",
    "            for evid in evid_buf:\n",
    "                if evid in predicted_pages:\n",
    "                    hit_count+=1\n",
    "            if hit_count == len(evid_buf):\n",
    "                at_least_get_one=True\n",
    "        hit_counts+=int(at_least_get_one)\n",
    "        count+=1\n",
    "    # Macro precision\n",
    "    print(f\"at least get one rate : {hit_counts / count}\")\n",
    "\n",
    "\n",
    "def calculate_recall(data,predictions: pd.Series) -> None:\n",
    "    recall = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        recall += len(hits) / len(gt_pages)\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Recall: {recall / count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d318ecda-a41d-488e-b3b9-9f972a6eb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_kfold_preds(pred_lst):\n",
    "    buf_dic = pred_lst[0]\n",
    "    for i in range(len(pred_lst)):\n",
    "        if i ==0:\n",
    "            continue\n",
    "        for key in pred_lst[i]:\n",
    "            if key in buf_dic.keys():\n",
    "                buf_dic[key]+=pred_lst[i][key]\n",
    "            else:\n",
    "                buf_dic[key]=pred_lst[i][key]\n",
    "    return buf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920e525d-dccf-4bfe-b203-5a0e65460b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3839"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_data = load_json(\"../preprocess/pre_train_doc100.jsonl\")\n",
    "len(ori_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "63315126-9324-488f-9558-ad7564059e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_preds=[]\n",
    "preds = [\"../pert_large/page/doc100_cluster_folds_recall/val_f0.json\",\"../pert_large/page/doc100_cluster_folds_recall/val_f1.json\"]\n",
    "for pred_path in preds:\n",
    "    pred = load_simple_json(pred_path)\n",
    "    processed_preds.append(process_prediction(pred,0.0015,None))\n",
    "paired_data = join_data_with_preds(ori_data,join_kfold_preds(processed_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8a11a69f-c6c6-461c-9be9-616544410fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.Series([set(elem[\"predicted_pages_1\"]) for elem in paired_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3e98ddc0-26de-4388-b6d9-801d892102cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.24516348320572232\n",
      "Recall: 0.8420076377523185\n",
      "at least get one rate : 0.7937806873977087\n"
     ]
    }
   ],
   "source": [
    "calculate_precision(paired_data,predictions)\n",
    "calculate_recall(paired_data,predictions)\n",
    "at_least_get_one(paired_data,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b8a64-2372-4f04-8fde-4cdfb5b5622f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d77eef-26ed-4bf8-bd0f-b38c690d8648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR_NLP",
   "language": "python",
   "name": "ir_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
