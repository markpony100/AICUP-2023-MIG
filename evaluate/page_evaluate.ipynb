{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee74834-cd91-46f4-b630-238ac54b272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import load_simple_json,load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9728443-2b4f-4fe1-b80c-8d86006d02b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_prediction(pred,thresh=0.5,topk=None):\n",
    "    '''\n",
    "    input: raw prediction\n",
    "    return: dictionary with id and page value\n",
    "    '''\n",
    "    re_dic={}\n",
    "    for qid in pred:\n",
    "        buf_lst=[]\n",
    "        scores = pred[qid][\"score\"]\n",
    "        if topk:\n",
    "            sorted_lst = sorted(scores,reverse=True)[:topk]\n",
    "            [buf_lst.append(pred[qid][\"page_ids\"][scores.index(i)]) for i in sorted_lst]\n",
    "        else:\n",
    "            [buf_lst.append(pred[qid][\"page_ids\"][idx]) for idx,value in \n",
    "                    enumerate(scores) if value >= thresh]\n",
    "        re_dic[qid]=buf_lst\n",
    "    return re_dic\n",
    "def join_data_with_preds(ori_data,preds,key = \"predicted_pages_1\"):\n",
    "    '''\n",
    "    input: original data and processed predcition\n",
    "    return: joint data with prediciton\n",
    "    '''\n",
    "    re_lst=[]\n",
    "    for i in ori_data:\n",
    "        id_buf = str(i[\"id\"])\n",
    "        if id_buf in preds.keys():\n",
    "            i[key]=preds[id_buf]\n",
    "            re_lst.append(i)\n",
    "    return re_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20dd55e6-b24b-4a59-bc83-65fac38f377f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_precision(data,predictions: pd.Series) -> None:\n",
    "    precision = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        # Extract all ground truth of titles of the wikipedia pages\n",
    "        # evidence[2] refers to the title of the wikipedia page\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        if len(predicted_pages) != 0:\n",
    "            precision += len(hits) / len(predicted_pages)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Macro precision\n",
    "    print(f\"Precision: {precision / count}\")\n",
    "def at_least_get_one(data,predictions: pd.Series) -> None:\n",
    "    precision = 0\n",
    "    count = 0\n",
    "    hit_counts=0\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        # Extract all ground truth of titles of the wikipedia pages\n",
    "        # evidence[2] refers to the title of the wikipedia page\n",
    "        at_least_get_one = False\n",
    "        for evidence_set in d[\"evidence\"]:\n",
    "            evid_buf=[]\n",
    "            for evidence in evidence_set:\n",
    "                evid_buf.append(evidence[2])\n",
    "            hit_count = 0\n",
    "            for evid in evid_buf:\n",
    "                if evid in predicted_pages:\n",
    "                    hit_count+=1\n",
    "            if hit_count == len(evid_buf):\n",
    "                at_least_get_one=True\n",
    "        hit_counts+=int(at_least_get_one)\n",
    "        count+=1\n",
    "    # Macro precision\n",
    "    print(f\"at least get one rate : {hit_counts / count}\")\n",
    "\n",
    "\n",
    "def calculate_recall(data,predictions: pd.Series) -> None:\n",
    "    recall = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "            continue\n",
    "\n",
    "        gt_pages = set([\n",
    "            evidence[2]\n",
    "            for evidence_set in d[\"evidence\"]\n",
    "            for evidence in evidence_set\n",
    "        ])\n",
    "        predicted_pages = predictions.iloc[i]\n",
    "        hits = predicted_pages.intersection(gt_pages)\n",
    "        recall += len(hits) / len(gt_pages)\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Recall: {recall / count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d318ecda-a41d-488e-b3b9-9f972a6eb54f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unique(input_lst):\n",
    "    elem_set = []\n",
    "    for elem in input_lst:\n",
    "        if elem not in elem_set:\n",
    "            elem_set.append(elem)\n",
    "    return elem_set\n",
    "def join_kfold_preds(pred_lst):\n",
    "    buf_dic = pred_lst[0]\n",
    "    for i in range(len(pred_lst)):\n",
    "        if i ==0:\n",
    "            continue\n",
    "        for key in pred_lst[i]:\n",
    "            if key in buf_dic.keys():\n",
    "                buf_dic[key]+=pred_lst[i][key]\n",
    "            else:\n",
    "                buf_dic[key]=pred_lst[i][key]\n",
    "    for key in buf_dic:#post process clean duplicate\n",
    "        buf_dic[key]=unique(buf_dic[key])\n",
    "    return buf_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920e525d-dccf-4bfe-b203-5a0e65460b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11349"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_data = load_json(\"../preprocess/pre_train_0522.jsonl\")\n",
    "len(ori_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63315126-9324-488f-9558-ad7564059e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_preds=[]\n",
    "preds = [\"../pert_large/page/0522_base_cluster/val_f0.json\"]\n",
    "thr_param =[]\n",
    "for pred_path in preds:\n",
    "    pred = load_simple_json(pred_path)\n",
    "    processed_preds.append(process_prediction(pred,0.005,None))\n",
    "paired_data = join_data_with_preds(ori_data,join_kfold_preds(processed_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a11a69f-c6c6-461c-9be9-616544410fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paired_data = [i for i in paired_data]\n",
    "predictions = pd.Series([set(elem[\"predicted_pages_1\"]) for elem in paired_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e98ddc0-26de-4388-b6d9-801d892102cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8921061318808289\n",
      "Recall: 0.9125024070864624\n",
      "at least get one rate : 0.8700173310225303\n"
     ]
    }
   ],
   "source": [
    "calculate_precision(paired_data,predictions)\n",
    "calculate_recall(paired_data,predictions)\n",
    "at_least_get_one(paired_data,predictions)\n",
    "#0.7855973813420621 Precision: 0.5449945444626291 f0 recall\n",
    "#0.7823240589198036 Precision: 0.5635077097843053 f0 f1\n",
    "#0.812111801242236 Precision: 0.5406277728482696 f1 recall\n",
    "#0.7839607201309329 Precision: 0.6245382277297165 f0 recall b14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81966bfb-77e6-4bf0-b806-fd47e5c72b17",
   "metadata": {},
   "source": [
    "preprocess page data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "c64123dc-77f2-4eef-ace1-2d75a006bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in paired_data:\n",
    "    i[\"predicted_pages_1\"]=list(set(i[\"predicted_pages_1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "ec5c0903-2f90-4c1b-82cc-cb0f2622077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../pert_large/page/doc100_cluster_folds_recall/all.jsonl\",\"w\",encoding=\"utf8\",) as f:\n",
    "                for i, d in enumerate(paired_data):\n",
    "                    f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf984b25-a885-4d4c-a673-93002007d2dd",
   "metadata": {},
   "source": [
    "evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "2006cdfc-b8bf-4e93-872e-7e5af0747d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11349"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_data = load_json(\"../preprocess/pre_all_0522.jsonl\")\n",
    "len(ori_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "c834ceb5-a38f-43c0-a5cb-fc6e24c69a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_preds=[]\n",
    "#preds = [\"no_info_f0\",\"no_info_f1\",\"no_info_f2\",\"no_info_f3\",\"no_info_f4\"]\n",
    "preds = [\"all_test_f0\",\"all_test_f1\",\"all_test_f2\",\"all_test_f3\",\"all_test_f4\"]\n",
    "#thr_param =[0.001, 0.001,0.004,0.004,]\n",
    "for idx,pred_path in enumerate(preds):\n",
    "    pred = load_simple_json(\"../pert_large/page/0522_base_clu_folds/no_info/\"+pred_path+\".json\")\n",
    "    processed_preds.append(process_prediction(pred,0.5,None))\n",
    "paired_data = join_data_with_preds(ori_data,join_kfold_preds(processed_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "fad4af9b-a9bd-41f6-81be-f4070347f10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3268"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "bb6d7fea-29bd-4555-841b-9c051e9848af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 8075,\n",
       "  'label': 'NOT ENOUGH INFO',\n",
       "  'claim': 'F.I.R.的團員有主唱Faye飛（詹雯婷）、吉他手Real阿沁（黃漢青）、鍵盤手Ian（陳建寧），是亞洲樂壇不常見的一女二男三人組合樂團。',\n",
       "  'evidence': [[7208, None, None, None]],\n",
       "  'predicted_pages_1': ['F.I.R.飛兒樂團']}]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "ec950835-28b8-4a9c-9123-391c4d5e8933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8458178527821373\n",
      "Recall: 0.9234787087912093\n",
      "at least get one rate : 0.8945054945054945\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "predictions = pd.Series([set(elem[\"predicted_pages_1\"]) for elem in paired_data])\n",
    "calculate_precision(paired_data,predictions)\n",
    "calculate_recall(paired_data,predictions)\n",
    "at_least_get_one(paired_data,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "a71ff0ee-2b33-4d91-be84-d065b6dff30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../pert_large/page/0522_base_clu_folds/no_info_ens5.jsonl\",\"w\",encoding=\"utf8\",) as f:\n",
    "    for i, d in enumerate(paired_data):\n",
    "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddde45-1205-4c0c-aef2-8f65248f53f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462349c7-49bf-4bf5-bd5c-2489b3b57758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f045c-02a1-4a0d-8706-ccd207b8fc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78720337-d076-4607-bce0-b51534c451ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15770279-0626-48bc-9bc4-4ec87507974e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b3b3bac-c6fd-44c8-85a3-8362fdb2b6c4",
   "metadata": {},
   "source": [
    "#new doc retriecal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b10b8a64-2372-4f04-8fde-4cdfb5b5622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_emb = load_json(\"../preprocess/pre_train_wikisearch_base.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62859d9b-920f-4062-bcda-bb92ffc570d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_preds = {str(i[\"id\"]):i[\"result\"] for i in wiki_emb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dd848e8-80fd-49d5-a2ab-ed7958054b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_data = join_data_with_preds(ori_data,join_kfold_preds([wiki_preds]),key=\"predicted_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ef923c2-9559-4861-9d2a-8c0a6eb49e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2663,\n",
       " 'label': 'refutes',\n",
       " 'claim': '天衛三軌道在天王星內部的磁層，以《仲夏夜之夢》作者緹坦妮雅命名。',\n",
       " 'evidence': [[[4209, 4331, '天衛三', 2]]],\n",
       " 'predicted_pages': ['天衛三',\n",
       "  '天衛四',\n",
       "  '天王星',\n",
       "  '天王星的衛星',\n",
       "  '仲夏夜之淫夢',\n",
       "  '天衛二',\n",
       "  '天衛一',\n",
       "  '天衛十',\n",
       "  '仲夏夜之夢_(消歧義)',\n",
       "  '緹坦妮雅']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2954309c-f1d4-489f-88b4-3a500fd2a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.Series([set(elem[\"predicted_pages\"]) for elem in paired_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d975046-0b0a-43f7-a1d5-3dfcd127a95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'仲夏夜之夢_(消歧義)',\n",
       " '仲夏夜之淫夢',\n",
       " '天王星',\n",
       " '天王星的衛星',\n",
       " '天衛一',\n",
       " '天衛三',\n",
       " '天衛二',\n",
       " '天衛十',\n",
       " '天衛四',\n",
       " '緹坦妮雅'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22809950-db41-46ee-9d8a-b541ecccd1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.10348846581817957\n",
      "Recall: 0.926340723309388\n",
      "at least get one rate : 0.9001733960862026\n"
     ]
    }
   ],
   "source": [
    "calculate_precision(paired_data,predictions)\n",
    "calculate_recall(paired_data,predictions)\n",
    "at_least_get_one(paired_data,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc58a68-1b9b-4490-b844-ac68fa76a061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR_NLP",
   "language": "python",
   "name": "ir_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
